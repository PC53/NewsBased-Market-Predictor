{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5vZ8LdF7mvir",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5vZ8LdF7mvir",
    "outputId": "bcdcc075-43be-48d6-b0b5-86263e41aa10"
   },
   "outputs": [],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ea7ec1",
   "metadata": {
    "id": "c0ea7ec1"
   },
   "outputs": [],
   "source": [
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext as sc\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark\n",
    "\n",
    "# tools\n",
    "import random\n",
    "import os\n",
    "import statistics\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00433a5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "00433a5c",
    "outputId": "065a8897-7414-4b06-d9e2-70ef0d3d4c13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java version \"11.0.16\" 2022-07-19 LTS\n",
      "Java(TM) SE Runtime Environment 18.9 (build 11.0.16+11-LTS-199)\n",
      "Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.16+11-LTS-199, mixed mode)\n"
     ]
    }
   ],
   "source": [
    "!java -version # java version 11.0.16 was used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd628f09",
   "metadata": {
    "id": "bd628f09"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765df6d9",
   "metadata": {
    "id": "765df6d9"
   },
   "source": [
    "# Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "994186ca",
   "metadata": {
    "id": "994186ca"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initialize Spark session object\n",
    "\"\"\"\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark Naive Bayes CountVectorizer\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18ab340c",
   "metadata": {
    "id": "18ab340c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+------+\n",
      "|            Datetime|               Title|         Description|             Content|Change|\n",
      "+--------------------+--------------------+--------------------+--------------------+------+\n",
      "|2019-01-01 20:56:...|Car sales languis...|Tight liquidity, ...|nonetight,liquidi...|  -1.2|\n",
      "|2019-01-01 18:07:...|Tata Motors domes...|The company said ...|nonethe,company,p...|  -1.2|\n",
      "|2019-01-02 20:30:...|An evening walk d...|At the close of m...|weak,global,cue,d...| -1.75|\n",
      "|2019-01-02 14:12:...|Top auto trends o...|The biggest news ...|automotive,indust...|  -1.2|\n",
      "|2019-01-04 16:18:...|No decision regar...|Economic Affairs ...|decision,recently...| -0.51|\n",
      "|2019-01-03 14:37:...|HDFC MF surpasses...|As of December-en...|hdfc,mutual,fund,...| -2.85|\n",
      "|2019-01-03 15:14:...|Axle load norms b...|Sales of medium a...|new,axle,load,nor...|  1.15|\n",
      "|2019-01-04 09:46:...|Allocate 30-40% o...|For aggressive in...|tell,select,midca...| -1.42|\n",
      "|2019-01-07 10:28:...|ONGC first gets f...|This situation ha...|capitalmindwe,big...|-19.84|\n",
      "|2019-01-07 14:55:...|Maggi fights to r...|In the latest twi...|nonemaggi,recoupe...| -7.05|\n",
      "|2019-01-07 17:26:...|10 years to Satya...|On January 7, 200...|january,byrraju,r...| -7.64|\n",
      "|2019-01-08 12:47:...|Future S Concept ...|With an all-new d...|maruti,suzuki,set...| -2.73|\n",
      "|2019-01-09 11:56:...|What to expect fr...|Features car buye...|muchloved,maruti,...| -3.09|\n",
      "|2019-01-11 09:34:...|TCS Q3 review: Ea...|We expect modest ...|moneycontrol,rese...| -8.43|\n",
      "|2019-01-10 18:37:...|Daimler’s BharatB...|Sales under the b...|nonesales,brand,b...| -3.35|\n",
      "|2019-01-11 18:12:...|Maruti Suzuki loo...|In its 35th year ...|nonein,th,year,un...| -2.47|\n",
      "|2019-01-11 15:39:...|2019 Maruti Suzuk...|An overview of on...|maruti,suzuki,hit...| -2.47|\n",
      "|2019-01-16 12:17:...|Maruti Suzuki rev...|While keeping the...|noneto,begin,maru...| -4.02|\n",
      "|2019-01-16 17:21:...|2019 Maruti Suzuk...|A brief overview ...|maruti,suzuki,off...| -3.55|\n",
      "|2019-02-05 17:42:...|Reliance Industri...|Jio's overall per...|none,nonenonenone...| -0.47|\n",
      "+--------------------+--------------------+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(os.path.join(os.getcwd(),\"..\",\"data/preprocessed_data.csv\"), header=True)\n",
    "# data = spark.read.csv(\"preprocessed_data.csv\", header=True)\n",
    "data.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a024bb2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a024bb2a",
    "outputId": "38dd90b6-0eb2-4ee1-ebc6-8f49a69309f5",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+------+-----+\n",
      "|            Datetime|               Title|         Description|             Content|Change|label|\n",
      "+--------------------+--------------------+--------------------+--------------------+------+-----+\n",
      "|2019-01-01 20:56:...|Car sales languis...|Tight liquidity, ...|[nonetight, liqui...|  -1.2|    0|\n",
      "|2019-01-01 18:07:...|Tata Motors domes...|The company said ...|[nonethe, company...|  -1.2|    0|\n",
      "|2019-01-02 20:30:...|An evening walk d...|At the close of m...|[weak, global, cu...| -1.75|    0|\n",
      "|2019-01-02 14:12:...|Top auto trends o...|The biggest news ...|[automotive, indu...|  -1.2|    0|\n",
      "|2019-01-04 16:18:...|No decision regar...|Economic Affairs ...|[decision, recent...| -0.51|    0|\n",
      "|2019-01-03 14:37:...|HDFC MF surpasses...|As of December-en...|[hdfc, mutual, fu...| -2.85|    0|\n",
      "|2019-01-03 15:14:...|Axle load norms b...|Sales of medium a...|[new, axle, load,...|  1.15|    1|\n",
      "|2019-01-04 09:46:...|Allocate 30-40% o...|For aggressive in...|[tell, select, mi...| -1.42|    0|\n",
      "|2019-01-07 10:28:...|ONGC first gets f...|This situation ha...|[capitalmindwe, b...|-19.84|    0|\n",
      "|2019-01-07 14:55:...|Maggi fights to r...|In the latest twi...|[nonemaggi, recou...| -7.05|    0|\n",
      "|2019-01-07 17:26:...|10 years to Satya...|On January 7, 200...|[january, byrraju...| -7.64|    0|\n",
      "|2019-01-08 12:47:...|Future S Concept ...|With an all-new d...|[maruti, suzuki, ...| -2.73|    0|\n",
      "|2019-01-09 11:56:...|What to expect fr...|Features car buye...|[muchloved, marut...| -3.09|    0|\n",
      "|2019-01-11 09:34:...|TCS Q3 review: Ea...|We expect modest ...|[moneycontrol, re...| -8.43|    0|\n",
      "|2019-01-10 18:37:...|Daimler’s BharatB...|Sales under the b...|[nonesales, brand...| -3.35|    0|\n",
      "|2019-01-11 18:12:...|Maruti Suzuki loo...|In its 35th year ...|[nonein, th, year...| -2.47|    0|\n",
      "|2019-01-11 15:39:...|2019 Maruti Suzuk...|An overview of on...|[maruti, suzuki, ...| -2.47|    0|\n",
      "|2019-01-16 12:17:...|Maruti Suzuki rev...|While keeping the...|[noneto, begin, m...| -4.02|    0|\n",
      "|2019-01-16 17:21:...|2019 Maruti Suzuk...|A brief overview ...|[maruti, suzuki, ...| -3.55|    0|\n",
      "|2019-02-05 17:42:...|Reliance Industri...|Jio's overall per...|[none, nonenoneno...| -0.47|    0|\n",
      "+--------------------+--------------------+--------------------+--------------------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Content was pre-processed and stored as comma seperated String\n",
    "'''\n",
    "def to_label(change):\n",
    "  try:\n",
    "    if change>0:\n",
    "      return 1\n",
    "    else:\n",
    "      return 0\n",
    "  except:\n",
    "    pass\n",
    "\n",
    "strList_toList = udf(lambda r: r.split(','), ArrayType(StringType()))\n",
    "func_tolabel = udf(lambda x : to_label(x) ,IntegerType())\n",
    "\n",
    "list_content = data.withColumn('Content',strList_toList('Content')).withColumn('label',col(\"Change\").cast(\"Float\")).na.drop(\"any\")\n",
    "list_content = list_content.withColumn('label',func_tolabel('label'))\n",
    "\n",
    "list_content.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JUE0zbGl4lxB",
   "metadata": {
    "id": "JUE0zbGl4lxB"
   },
   "source": [
    "# CountVectoriser Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef652860",
   "metadata": {
    "id": "ef652860"
   },
   "source": [
    "### Vectorise the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1cd4984",
   "metadata": {
    "id": "c1cd4984"
   },
   "outputs": [],
   "source": [
    "to_vectorize = list_content.select('Datetime','Content', 'label')\n",
    "cv = CountVectorizer(inputCol=\"Content\", outputCol=\"features\")\n",
    "\n",
    "model_vec = cv.fit(to_vectorize)\n",
    "result_vec = model_vec.transform(to_vectorize)\n",
    "selectedData = result_vec.select('features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9136e5c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d9136e5c",
    "outputId": "65decce0-ff06-4cc0-8211-0be733d9c344",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: int]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selectedData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7d73e",
   "metadata": {
    "id": "91b7d73e"
   },
   "source": [
    "### Building Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cce45fa",
   "metadata": {
    "id": "5cce45fa"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define TruePositive, FalsePositive and FalseNegative\n",
    "x = prediction, y = label\n",
    "\"\"\"\n",
    "TP = udf(lambda x,y: int(x==1 and y==1))\n",
    "FP = udf(lambda x,y: int(x==1 and y==0))\n",
    "FN = udf(lambda x,y: int(x==0 and y==1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77cbac69",
   "metadata": {
    "id": "77cbac69"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Naive-Bayes following from CountVectorizer\n",
    "'''\n",
    "def NAIVEBAYES_CV(smooth=1, model_type=\"multinomial\"): \n",
    "    \n",
    "    # separating train/test data\n",
    "    # taking equal number of positive and negative changes while splitting\n",
    "    training_negative, test_negative = selectedData.where(selectedData.label == 0).randomSplit([0.7, 0.3])\n",
    "    training_positive, test_positive = selectedData.where(selectedData.label == 1).randomSplit([0.7, 0.3])\n",
    "\n",
    "    training = training_positive.union(training_negative)\n",
    "    test = test_negative.union(test_positive)\n",
    "\n",
    "    # create trainer with parameters then train\n",
    "    # smoothing: smooth probabilities of 0 to the input\n",
    "    nb = NaiveBayes(smoothing=smooth, modelType=model_type)\n",
    "    model_NB = nb.fit(training)\n",
    "\n",
    "    # display on test set: appends a prediction column\n",
    "    predictions = model_NB.transform(test)\n",
    "    \n",
    "    # diagnostic testing\n",
    "    prela_df = predictions.select(\"prediction\",\"label\")\n",
    "    prela_df=prela_df.withColumn(\"TP\", TP(prela_df.prediction,prela_df.label))\n",
    "    prela_df=prela_df.withColumn(\"FP\", FP(prela_df.prediction,prela_df.label))\n",
    "    prela_df=prela_df.withColumn(\"FN\", FN(prela_df.prediction,prela_df.label))\n",
    "    \n",
    "#     return prela_df\n",
    "\n",
    "    TP_ = prela_df.where(prela_df.TP==1).count()\n",
    "    FP_ = prela_df.where(prela_df.FP==1).count()\n",
    "    FN_ = prela_df.where(prela_df.FN==1).count()\n",
    "\n",
    "    precision = TP_/(TP_+FP_)\n",
    "    recall = TP_/(TP_+FN_)\n",
    "    F1 = 2*(precision*recall)/(precision+recall)\n",
    "\n",
    "    # compute accuracy of on test set: compares labelCol and predictionCol\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "    accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "    # return test results and model object\n",
    "    return (accuracy,precision,recall,F1,model_NB,prela_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30066f75",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30066f75",
    "outputId": "ead899c4-0a1f-41cd-f178-2b3ac8589741",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Manually trying few examples\n",
    "\n",
    "acc, precision, recall, F1, modelNB1, prela_df1 = NAIVEBAYES_CV(0.2684835187532758,\"multinomial\")\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fa0a3f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "collapsed": true,
    "id": "0fa0a3f5",
    "outputId": "2d33a41a-dd25-4ce4-975e-f7de791394c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Mean_acc:  0.7766455295860981  => Mean_f1:  0.25787998569839504 - Smoothing: 0.5988610787445499 - Model: complement\n",
      "=> Mean_acc:  0.749400271208196  => Mean_f1:  0.243141301784549 - Smoothing: 0.11283032778252543 - Model: complement\n",
      "=> Mean_acc:  0.7558577164258549  => Mean_f1:  0.2592191839228742 - Smoothing: 0.04807813441729131 - Model: complement\n",
      "=> Mean_acc:  0.7484376146777002  => Mean_f1:  0.23867267803335293 - Smoothing: 0.5097049072357274 - Model: complement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\chhal\\documents\\discord_bot\\newsbased-market-predictor\\newspredictorenv\\lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"c:\\users\\chhal\\documents\\discord_bot\\newsbased-market-predictor\\newspredictorenv\\lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"C:\\Users\\chhal\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m smoothing \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39muniform(\u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iter_each):\n\u001b[1;32m---> 20\u001b[0m     acc,precision,recall,F1,modelNB,prela_df \u001b[38;5;241m=\u001b[39m \u001b[43mNAIVEBAYES_CV\u001b[49m\u001b[43m(\u001b[49m\u001b[43msmoothing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     accs\u001b[38;5;241m.\u001b[39mappend(acc)\n\u001b[0;32m     22\u001b[0m     f1s\u001b[38;5;241m.\u001b[39mappend(F1)\n",
      "Cell \u001b[1;32mIn [10], line 17\u001b[0m, in \u001b[0;36mNAIVEBAYES_CV\u001b[1;34m(smooth, model_type)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# create trainer with parameters then train\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# smoothing: smooth probabilities of 0 to the input\u001b[39;00m\n\u001b[0;32m     16\u001b[0m nb \u001b[38;5;241m=\u001b[39m NaiveBayes(smoothing\u001b[38;5;241m=\u001b[39msmooth, modelType\u001b[38;5;241m=\u001b[39mmodel_type)\n\u001b[1;32m---> 17\u001b[0m model_NB \u001b[38;5;241m=\u001b[39m \u001b[43mnb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# display on test set: appends a prediction column\u001b[39;00m\n\u001b[0;32m     20\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model_NB\u001b[38;5;241m.\u001b[39mtransform(test)\n",
      "File \u001b[1;32mc:\\users\\chhal\\documents\\discord_bot\\newsbased-market-predictor\\newspredictorenv\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32mc:\\users\\chhal\\documents\\discord_bot\\newsbased-market-predictor\\newspredictorenv\\lib\\site-packages\\pyspark\\ml\\wrapper.py:379\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 379\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32mc:\\users\\chhal\\documents\\discord_bot\\newsbased-market-predictor\\newspredictorenv\\lib\\site-packages\\pyspark\\ml\\wrapper.py:376\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\chhal\\documents\\discord_bot\\newsbased-market-predictor\\newspredictorenv\\lib\\site-packages\\py4j\\java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32mc:\\users\\chhal\\documents\\discord_bot\\newsbased-market-predictor\\newspredictorenv\\lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32mc:\\users\\chhal\\documents\\discord_bot\\newsbased-market-predictor\\newspredictorenv\\lib\\site-packages\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Iteration tests on Naive-Bayes\n",
    "\n",
    "iter_total: iterations for different smoothing nb\n",
    "iter_each: iterations for the same smoothing nb\n",
    "'''\n",
    "\n",
    "extract_method = \"CountVectorizer\"\n",
    "iter_each = 10\n",
    "iter_total = 5\n",
    "m_types = [\"complement\", \"multinomial\"]\n",
    "accs = []\n",
    "f1s = []\n",
    "means = []\n",
    "for model_type in m_types:\n",
    "    for k in range(iter_total):\n",
    "        accuracies = []\n",
    "        smoothing = random.uniform(0.01, 0.8)\n",
    "        for i in range(iter_each):\n",
    "            acc,precision,recall,F1,modelNB,prela_df = NAIVEBAYES_CV(smoothing, model_type)\n",
    "            accs.append(acc)\n",
    "            f1s.append(F1)\n",
    "        mean_acc = statistics.mean(accs)\n",
    "        mean_f1 = statistics.mean(f1s)\n",
    "        print(\"=> Mean_acc: \", mean_acc,\" => Mean_f1: \",mean_f1, \"- Smoothing:\", smoothing, \"- Model:\", model_type)\n",
    "        means.append((mean_acc,mean_f1, smoothing, model_type, extract_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cs0LrzRK66rJ",
   "metadata": {
    "id": "cs0LrzRK66rJ"
   },
   "outputs": [],
   "source": [
    "acc_df = pd.DataFrame(means, columns=['mean','F1', 'smoothing', 'model_type', 'extract_method'])\n",
    "acc_df.to_csv(os.path.join(os.path.dirname(os.getcwd()),\"data/means_count.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KugiLWwu7vTD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "KugiLWwu7vTD",
    "outputId": "17702656-cf04-4e65-bba0-d7ea1c9751b7"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.download('means_count.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bead34",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b257b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(os.path.dirname(os.getcwd()),'saved_models/cvnb_model')\n",
    "\n",
    "## saving the highest accuracy model \n",
    "max_acc = acc_df.loc[acc_df['mean'].idxmax()]\n",
    "smoothing = max_acc.smoothing \n",
    "model_type = max_acc.model_type\n",
    "\n",
    "(_, _, _, _, final_model, _) = NAIVEBAYES_CV(smooth=smoothing, model_type=model_type) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d2bf744e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o7559.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.classification.NaiveBayesModel$NaiveBayesModelWriter.saveImpl(NaiveBayes.scala:571)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$3(SparkHadoopWriter.scala:100)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\t... 51 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mfinal_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\chhal\\documents\\discord_bot\\newsbased-market-predictor\\newspredictorenv\\lib\\site-packages\\pyspark\\ml\\util.py:246\u001b[0m, in \u001b[0;36mMLWritable.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;124;03m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\chhal\\documents\\discord_bot\\newsbased-market-predictor\\newspredictorenv\\lib\\site-packages\\pyspark\\ml\\util.py:197\u001b[0m, in \u001b[0;36mJavaMLWriter.save\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[1;32m--> 197\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\chhal\\documents\\discord_bot\\newsbased-market-predictor\\newspredictorenv\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mc:\\users\\chhal\\documents\\discord_bot\\newsbased-market-predictor\\newspredictorenv\\lib\\site-packages\\pyspark\\sql\\utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\users\\chhal\\documents\\discord_bot\\newsbased-market-predictor\\newspredictorenv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o7559.save.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:106)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1091)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1089)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1062)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1027)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1009)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1008)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:965)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:963)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1599)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1599)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1585)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1585)\r\n\tat org.apache.spark.ml.util.DefaultParamsWriter$.saveMetadata(ReadWrite.scala:413)\r\n\tat org.apache.spark.ml.classification.NaiveBayesModel$NaiveBayesModelWriter.saveImpl(NaiveBayes.scala:571)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:168)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.hadoop.mapred.FileOutputCommitter.commitJob(FileOutputCommitter.java:136)\r\n\tat org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:192)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.$anonfun$write$3(SparkHadoopWriter.scala:100)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:642)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\t... 51 more\r\n"
     ]
    }
   ],
   "source": [
    "final_model.write().overwrite().save(model_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "newsPredictorEnv",
   "language": "python",
   "name": "newspredictorenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "2b93f37cf5688af264e3b2dd81336fc0fb8079d2fdf86df8861846192838153f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
